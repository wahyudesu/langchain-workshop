{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c79ff15",
   "metadata": {},
   "source": [
    "## Using LLM Model in Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50321097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don‚Äôt skeletons fight each other?\n",
      "\n",
      "They don‚Äôt have the guts.\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatGroq(\n",
    "    model=\"openai/gpt-oss-20b\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "response = model.invoke(\"Tell me a joke\")\n",
    "# print(response)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b922eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "model = ChatGroq(\n",
    "    model=\"openai/gpt-oss-20b\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "system_msg = SystemMessage(\n",
    "    \"You are a helpful assistant that responds to questions with three exclamation marks.\"\n",
    ")\n",
    "human_msg = HumanMessage(\"What is the capital of France?\")\n",
    "\n",
    "response = model.invoke([system_msg, human_msg])\n",
    "# print(response)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db85d041",
   "metadata": {},
   "source": [
    "`HumanMessage`\n",
    "A message sent from the perspective of the human, with the user role\n",
    "\n",
    "`AIMessage`\n",
    "A message sent from the perspective of the AI that the human is interacting with, with the assistant role\n",
    "\n",
    "`SystemMessage`\n",
    "A message setting the instructions the AI should follow, with the system role\n",
    "\n",
    "`ChatMessage`\n",
    "A message allowing for arbitrary setting of role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bfe2205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Answer the question based on the\\n    context below. If the question cannot be answered using the information \\n    provided, answer with \"I don\\'t know\".\\n\\nContext: The most recent advancements in NLP are being driven by Large \\n        Language Models (LLMs). These models outperform their smaller \\n        counterparts and have become invaluable for developers who are creating \\n        applications with NLP capabilities. Developers can tap into these \\n        models through Hugging Face\\'s `transformers` library, or by utilizing \\n        OpenAI and Cohere\\'s offerings through the `openai` and `cohere` \\n        libraries, respectively.\\n\\nQuestion: Which model providers offer LLMs?\\n\\nAnswer: ')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_template(\"\"\"Answer the question based on the\n",
    "    context below. If the question cannot be answered using the information \n",
    "    provided, answer with \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \"\"\")\n",
    "\n",
    "template.invoke({\n",
    "    \"context\": \"\"\"The most recent advancements in NLP are being driven by Large \n",
    "        Language Models (LLMs). These models outperform their smaller \n",
    "        counterparts and have become invaluable for developers who are creating \n",
    "        applications with NLP capabilities. Developers can tap into these \n",
    "        models through Hugging Face's `transformers` library, or by utilizing \n",
    "        OpenAI and Cohere's offerings through the `openai` and `cohere` \n",
    "        libraries, respectively.\"\"\",\n",
    "    \"question\": \"Which model providers offer LLMs?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ecc876",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system', '''Answer the question based on the context below. If the \n",
    "        question cannot be answered using the information provided, answer with \n",
    "        \"I don\\'t know\".'''),\n",
    "    ('human', 'Context: {context}'),\n",
    "    ('human', 'Question: {question}'),\n",
    "])\n",
    "\n",
    "template.invoke({\n",
    "    \"context\": \"\"\"The most recent advancements in NLP are being driven by Large \n",
    "        Language Models (LLMs). These models outperform their smaller \n",
    "        counterparts and have become invaluable for developers who are creating \n",
    "        applications with NLP capabilities. Developers can tap into these \n",
    "        models through Hugging Face's `transformers` library, or by utilizing \n",
    "        OpenAI and Cohere's offerings through the `openai` and `cohere` \n",
    "        libraries, respectively.\"\"\",\n",
    "    \"question\": \"Which model providers offer LLMs?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7fd6a3",
   "metadata": {},
   "source": [
    "Prompt community\n",
    "\n",
    "https://smith.langchain.com/hub/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10096fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LANGSMITH_API_KEY in Settings > API Keys\n",
    "from langsmith import Client\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "client = Client(api_key=os.environ.get(\"LANGSMITH_API_KEY\"))\n",
    "prompt = client.pull_prompt(\"hardkothari/prompt-maker\", include_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6cf8dc",
   "metadata": {},
   "source": [
    "## Specific Formats out of LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae12f51d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnswerWithJustification(answer='They weigh the same ‚Äì both are one pound.', justification='A pound is a unit of mass, so a pound of bricks and a pound of feathers each have the same mass of one pound. The difference is in volume and density, not weight.')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class AnswerWithJustification(BaseModel):\n",
    "    '''An answer to the user's question along with justification for the \n",
    "        answer.'''\n",
    "    answer: str\n",
    "    '''The answer to the user's question'''\n",
    "    justification: str\n",
    "    '''Justification for the answer'''\n",
    "\n",
    "model = ChatGroq(\n",
    "    model=\"openai/gpt-oss-20b\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "structured_llm = model.with_structured_output(AnswerWithJustification)\n",
    "\n",
    "structured_llm.invoke(\"\"\"What weighs more, a pound of bricks or a pound \n",
    "    of feathers\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa42217",
   "metadata": {},
   "source": [
    "## Runnable Interface\n",
    "There is a common interface with these methods:\n",
    "\n",
    "`invoke`: transforms a single input into an output\n",
    "\n",
    "`batch`: efficiently transforms multiple inputs into multiple outputs\n",
    "\n",
    "`stream`: streams output from a single input as it‚Äôs produced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b02d363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! üëã How can I help you today?'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatGroq(\n",
    "    model=\"openai/gpt-oss-20b\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "completion = model.invoke('Hi there!') \n",
    "# Hi!\n",
    "\n",
    "# completions = model.batch(['Hi there!', 'Bye!'])\n",
    "# # ['Hi!', 'See you!']\n",
    "\n",
    "# for token in model.stream('Bye!'):\n",
    "#     print(token.content)\n",
    "#     # Good\n",
    "#     # bye\n",
    "#     # !\n",
    "\n",
    "completion.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e47f0274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! üëã How can I help you today?'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completions = model.batch(['Hi there!', 'Bye!'])\n",
    "\n",
    "completion.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1763fd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Why\n",
      " don\n",
      "‚Äôt\n",
      " skeleton\n",
      "s\n",
      " fight\n",
      " each\n",
      " other\n",
      "?\n",
      "\n",
      "\n",
      "They\n",
      " don\n",
      "‚Äôt\n",
      " have\n",
      " the\n",
      " guts\n",
      "!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token in model.stream('Tell me a joke!'):\n",
    "    # IMPORTANT: Keep the processing of each chunk as efficient as possible.\n",
    "    # While you're processing the current chunk, the upstream component is\n",
    "    # waiting to produce the next one. For example, if working with LangGraph,\n",
    "    # graph execution is paused while the current chunk is being processed.\n",
    "    # In extreme cases, this could even result in timeouts (e.g., when llm outputs are\n",
    "    # streamed from an API that has a timeout).\n",
    "    print(token.content)\n",
    "    # Good\n",
    "    # bye\n",
    "    # !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691102e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the parrot bring did the parrot bring a ladder to the bar?\n",
      "\n",
      " a ladder to the bar?\n",
      "\n",
      "Because it heard the drinksBecause it heard the drinks were on the house! ü¶ú were on the house! ü¶úüçπüçπ"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = ChatGroq(\n",
    "    model=\"openai/gpt-oss-20b\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "parser = StrOutputParser()\n",
    "chain = prompt | model | parser\n",
    "\n",
    "async for event in chain.astream_events({\"topic\": \"parrot\"}):\n",
    "    kind = event[\"event\"]\n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        # Ambil content dari chunk\n",
    "        content = event[\"data\"][\"chunk\"].content\n",
    "        if content:\n",
    "            print(content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92ccbe8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
