{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2ae07a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "let GROQ_API_KEY = Deno.env.get(\"GROQ_API_KEY\");\n",
    "if (!GROQ_API_KEY) {\n",
    "  const input = typeof prompt === \"function\" ? prompt(\"Enter GROQ_API_KEY:\") : null;\n",
    "  if (input) {\n",
    "    Deno.env.set(\"GROQ_API_KEY\", input);\n",
    "    GROQ_API_KEY = input;\n",
    "    // Tidak menampilkan API key ke console\n",
    "  } else {\n",
    "    throw new Error(\"GROQ_API_KEY not provided. Set it in the environment or provide it interactively.\");\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc8c3a5",
   "metadata": {},
   "source": [
    "## Using LLM Model in Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39be048c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "What do you call a fake noodle?\n",
      "\n",
      "An impasta!\n",
      "\n",
      "Hope that made you laugh! Do you want to hear another one?\n"
     ]
    }
   ],
   "source": [
    "import { ChatGroq } from \"@langchain/groq\";\n",
    "\n",
    "const model = new ChatGroq({model: \"llama-3.3-70b-versatile\"});\n",
    "\n",
    "const response = await model.invoke('Tell me a joke');\n",
    "\n",
    "// console.log(response);\n",
    "console.log(response.content);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72439539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "import { ChatGroq } from \"@langchain/groq\";\n",
    "import { HumanMessage, SystemMessage } from '@langchain/core/messages';\n",
    "\n",
    "const model = new ChatGroq({model: \"llama-3.3-70b-versatile\"});\n",
    "const prompt = [\n",
    "  new SystemMessage(\n",
    "    'You are a helpful assistant that responds to questions with three exclamation marks.'\n",
    "  ),\n",
    "  new HumanMessage('What is the capital of France?'),\n",
    "];\n",
    "\n",
    "const response = await model.invoke(prompt);\n",
    "\n",
    "console.log(response.content);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1861f92c",
   "metadata": {},
   "source": [
    "`HumanMessage`\n",
    "A message sent from the perspective of the human, with the user role\n",
    "\n",
    "`AIMessage`\n",
    "A message sent from the perspective of the AI that the human is interacting with, with the assistant role\n",
    "\n",
    "`SystemMessage`\n",
    "A message setting the instructions the AI should follow, with the system role\n",
    "\n",
    "`ChatMessage`\n",
    "A message allowing for arbitrary setting of role"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccde5238",
   "metadata": {},
   "source": [
    "## Making LLM Prompts Reusable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7f67957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StringPromptValue {\n",
      "  lc_serializable: true,\n",
      "  lc_kwargs: {\n",
      "    value: `Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don't know\".\\n` +\n",
      "      \"\\n\" +\n",
      "      \"Context: The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\\n\" +\n",
      "      \"\\n\" +\n",
      "      \"Question: Which model providers offer LLMs?\\n\" +\n",
      "      \"\\n\" +\n",
      "      \"Answer: \"\n",
      "  },\n",
      "  lc_namespace: [ \"langchain_core\", \"prompt_values\" ],\n",
      "  value: `Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don't know\".\\n` +\n",
      "    \"\\n\" +\n",
      "    \"Context: The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\\n\" +\n",
      "    \"\\n\" +\n",
      "    \"Question: Which model providers offer LLMs?\\n\" +\n",
      "    \"\\n\" +\n",
      "    \"Answer: \"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import { PromptTemplate } from '@langchain/core/prompts';\n",
    "\n",
    "const template =\n",
    "  PromptTemplate.fromTemplate(`Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: `);\n",
    "\n",
    "const response = await template.invoke({\n",
    "  context:\n",
    "    \"The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\",\n",
    "  question: 'Which model providers offer LLMs?',\n",
    "});\n",
    "\n",
    "console.log(response);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aad1e219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI and Cohere are mentioned as model providers that offer LLMs, in addition to Hugging Face.\n"
     ]
    }
   ],
   "source": [
    "import { PromptTemplate } from '@langchain/core/prompts';\n",
    "import { ChatGroq } from \"@langchain/groq\";\n",
    "\n",
    "const model = new ChatGroq({model: \"llama-3.3-70b-versatile\"});\n",
    "\n",
    "const template =\n",
    "  PromptTemplate.fromTemplate(`Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: `);\n",
    "\n",
    "const prompt = await template.invoke({\n",
    "  context:\n",
    "    \"The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\",\n",
    "  question: 'Which model providers offer LLMs?',\n",
    "});\n",
    "\n",
    "const response = await model.invoke(prompt);\n",
    "console.log(response.content);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d79253b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI and Cohere are mentioned as model providers that offer LLMs, alongside Hugging Face, which provides access to these models through its `transformers` library.\n"
     ]
    }
   ],
   "source": [
    "import { ChatPromptTemplate } from '@langchain/core/prompts';\n",
    "import { ChatGroq } from \"@langchain/groq\";\n",
    "\n",
    "const model = new ChatGroq({model: \"llama-3.3-70b-versatile\"});\n",
    "\n",
    "const template = ChatPromptTemplate.fromMessages([\n",
    "  [\n",
    "    'system',\n",
    "    'Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don\\'t know\".',\n",
    "  ],\n",
    "  ['human', 'Context: {context}'],\n",
    "  ['human', 'Question: {question}'],\n",
    "]);\n",
    "\n",
    "const prompt = await template.invoke({\n",
    "  context:\n",
    "    \"The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\",\n",
    "  question: 'Which model providers offer LLMs?',\n",
    "});\n",
    "\n",
    "const response = await model.invoke(prompt);\n",
    "console.log(response.content);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b76eb58",
   "metadata": {},
   "source": [
    "Prompt community\n",
    "\n",
    "https://smith.langchain.com/hub/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dadca4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set the LANGSMITH_API_KEY environment variable (create key in Settings > API Keys)\n",
    "\n",
    "// If you are in a non-Node environment, please use the default \"langchain/hub\" entrypoint and omit includeModel for providers other than OpenAI\n",
    "import * as hub from \"langchain/hub/node\";\n",
    "\n",
    "await hub.pull(\"hardkothari/prompt-maker\", {\n",
    "  includeModel: true\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5e32ea",
   "metadata": {},
   "source": [
    "## Specific Formats out of LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e84f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatGroq } from \"@langchain/groq\";\n",
    "import { z } from \"zod\";\n",
    "\n",
    "const answerSchema = z\n",
    "  .object({\n",
    "    answer: z.string().describe(\"The answer to the user's question\"),\n",
    "    justification: z.string().describe(`Justification for the \n",
    "      answer`),\n",
    "  })\n",
    "  .describe(`An answer to the user's question along with justification for \n",
    "    the answer.`);\n",
    "\n",
    "\n",
    "const model = new ChatGroq({model: \"llama-3.3-70b-versatile\"}).withStructuredOutput(answerSchema)\n",
    "\n",
    "await model.invoke(\"What weighs more, a pound of bricks or a pound of feathers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7eaa38",
   "metadata": {},
   "source": [
    "## Runnable Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75974b0",
   "metadata": {},
   "source": [
    "There is a common interface with these methods:\n",
    "\n",
    "`invoke`: transforms a single input into an output\n",
    "\n",
    "`batch`: efficiently transforms multiple inputs into multiple outputs\n",
    "\n",
    "`stream`: streams output from a single input as it’s produced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df341523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's nice to meet you. Is there something I can help you with or would you like to chat?\n",
      "\n",
      "By\n",
      "e\n",
      "!\n",
      " It\n",
      "\n",
      "By\n",
      "e\n",
      "!\n",
      " It\n",
      " was\n",
      " nice\n",
      " chatting\n",
      " with\n",
      " you\n",
      ".\n",
      " Have\n",
      " a\n",
      " great\n",
      " day\n",
      "!\n",
      "\n",
      "\n",
      " was\n",
      " nice\n",
      " chatting\n",
      " with\n",
      " you\n",
      ".\n",
      " Have\n",
      " a\n",
      " great\n",
      " day\n",
      "!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import { ChatGroq } from \"@langchain/groq\";\n",
    "\n",
    "const model = new ChatGroq({model: \"llama-3.3-70b-versatile\"});\n",
    "\n",
    "const response = await model.invoke('Hi there!');\n",
    "console.log(response.content);\n",
    "// Hi!\n",
    "\n",
    "const completions = await model.batch(['Hi there!', 'Bye!']);\n",
    "// ['Hi!', 'See you!']\n",
    "\n",
    "for await (const token of await model.stream('Bye!')) {\n",
    "  console.log(token.content);\n",
    "  // Good\n",
    "  // bye\n",
    "  // !\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "codemirror_mode": "typescript",
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nbconvert_exporter": "script",
   "pygments_lexer": "typescript",
   "version": "5.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
